{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning- COIY065H7\n",
    "\n",
    "## Coursework, WAME Optimiser Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load modules for use throughout the workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import Module\n",
    "from torch.optim import *\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchsummary import summary\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify versions of ML libraries being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0\n",
      "10.2\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routines to be used \n",
    "\n",
    "PrintResults will output a table of Precision and Recall for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrintResults(predictions, actuals):\n",
    "    ynp = predictions.numpy()\n",
    "    testnp = actuals.numpy()\n",
    "    print(\"Class \\t Cases \\t TP \\t FP \\t Precision \\t Recall\")\n",
    "    allcases = 0\n",
    "    allTP = 0\n",
    "    allFP = 0\n",
    "    for i in range(0,6):\n",
    "        cases = sum(testnp==i)\n",
    "        allcases += cases\n",
    "        TP = sum((ynp==i) & (testnp==i))\n",
    "        allTP += TP\n",
    "        FP = sum((ynp==i) & (testnp!=i))\n",
    "        allFP += FP\n",
    "        Precision = TP / (TP + FP + 0.000000001)\n",
    "        Recall = TP / cases\n",
    "        print(i, \"\\t\", cases, \"\\t\" ,TP, \"\\t\", FP, \"\\t\", round(Precision, 3), \"\\t\", \"\\t\", round(Recall, 3))\n",
    "    Precision = allTP / (allTP + allFP + 0.000000001)\n",
    "    Recall = allTP / allcases\n",
    "    print(\"Total\", \"\\t\", allcases, \"\\t\" ,allTP, \"\\t\", allFP, \"\\t\", round(Precision, 3), \"\\t\", \"\\t\", round(Recall, 3))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check returns the class predictions and accuracy score of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Check(Xvals, Yactuals, model):\n",
    "    predictraw = model(Xvals.to(device)).detach().cpu()\n",
    "    predict = np.argmax(predictraw, axis=-1)\n",
    "    predict_score = predict == Yactuals\n",
    "    correct = np.sum(predict_score.numpy(), axis=-1)\n",
    "    sumtot = predict_score.shape[0]\n",
    "    score = correct / sumtot\n",
    "    return (predict, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wame optimizer\n",
    "\n",
    "Logic for the optimiser follows the paper by Mosca, \n",
    "\n",
    "A. Mosca and G. D. Magoulas, ‘Training convolutional networks with weight-wise adaptive learning rates’, in ESANN, 2017.\n",
    "\n",
    "The code for the PyTorch optimizer implementation was adapted from the PyTorch souce code using -\n",
    "\n",
    "PyTorch optimizer class code: https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "Example of RProp implementation: https://pytorch.org/docs/stable/_modules/torch/optim/rprop.html\n",
    "\n",
    "And guidance on writing a custom optimizer for AdamW: http://mcneela.github.io/machine_learning/2019/09/03/Writing-Your-Own-Optimizers-In-Pytorch.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wame(Optimizer):\n",
    "\n",
    "\n",
    "    def __init__(self, params,lr=1e-3, alpha=0.9, etas=(0.1, 1.2), zetas=(0.01, 100), epsilon=1e-10, debug=False ):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= alpha:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(alpha))\n",
    "        if not 0.0 < etas[0] < 1.0 < etas[1]:\n",
    "            raise ValueError(\"Invalid eta values: {}, {}\".format(etas[0], etas[1]))\n",
    "\n",
    "        defaults = dict(alpha=alpha, etas=etas, zetas=zetas, lr=lr, epsilon=epsilon, debug=debug)\n",
    "        super(wame, self).__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "   \n",
    "        for group in self.param_groups:    \n",
    "            etaminus, etaplus = group['etas']\n",
    "            zeta_min, zeta_max = group['zetas']\n",
    "            alpha = group['alpha']\n",
    "            lr = group['lr']\n",
    "            epsilon = group['epsilon']\n",
    "            debug = group['debug']\n",
    "            _p = 0\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('wame does not support sparse gradients')\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['prev'] = torch.ones_like(p.data, memory_format=torch.preserve_format)\n",
    "                    state['prev'] = grad\n",
    "                    state['Theta'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n",
    "                    state['Z'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n",
    "                    state['zeta'] = torch.ones_like(p.data, memory_format=torch.preserve_format)\n",
    "                    state['gradmult'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n",
    "                    \n",
    "                    state['step_size'] = grad.new().resize_as_(grad).fill_(group['lr']) ### check on initialisation\n",
    "                    state['prev'] = grad\n",
    "                    \n",
    "                if (_p == 0) & debug:\n",
    "                    print(\"Weights at start: \",p.data.sum(), p.data.size())\n",
    "                step_size = state['step_size']\n",
    "                Theta = state['Theta']\n",
    "                Z = state['Z']\n",
    "                zeta = state['zeta']\n",
    "                gradmult = state['gradmult']                    \n",
    "                \n",
    "                state['step'] += 1\n",
    "\n",
    "                gradmult = grad.mul(state['prev'])\n",
    "                zeta[gradmult.gt(0.)] = zeta[gradmult.gt(0.)].mul(etaplus).clamp(zeta_min, zeta_max)\n",
    "                zeta[gradmult.lt(0.)] = zeta[gradmult.lt(0.)].mul(etaminus).clamp(zeta_min, zeta_max)\n",
    "                zeta[gradmult.eq(0.)] = 1\n",
    "                \n",
    "                Z = Z.mul(alpha).add(zeta.mul(1 - alpha))\n",
    "                Theta = Theta.mul(alpha).add(grad.mul(grad).mul(1 - alpha))\n",
    "\n",
    "                step_size = Z.mul(-lr).mul(grad).div(Theta.add(epsilon))\n",
    "\n",
    "                grad = grad.clone(memory_format=torch.preserve_format)\n",
    "                \n",
    "                # update parameters\n",
    "                p.data = p.data.add(step_size)\n",
    "                \n",
    "                if (_p == 0) & debug:\n",
    "                    print(\"Step number \", state['step'])\n",
    "                    print(\"Weights after update: \",p.data.sum(), p.data.size())\n",
    "                    print(\"Updates applied: \",step_size.sum(), step_size.size())\n",
    "                    print(\"Theta: \",Theta.sum())\n",
    "                    print(\"Z: \",Z.sum())\n",
    "                    print(\"zeta: \",zeta.sum())\n",
    "                    print(\"grad: \",grad.sum())\n",
    "                    print(\"grad_sqd\", grad_sqd.sum())\n",
    "                    print(\"gradmult\", gradmult.sum())\n",
    "                state['prev'] = grad\n",
    "                \n",
    "                _p = _p + 1\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAbbElEQVR4nO3de5QlZX3u8e/DTQQEFEaEGWS8EIy6FkhGlHBCCBgVNYEcJdEQJQRFDOageIJoEjHGuIgmxksUQxwEEy/gLRBFxYAc0ATigBBuIgQRBlAGGa6GyOV3/qi3sWl6uvYMvXvvpr+ftfbqXW+9VfXr3bv76XqrdlWqCkmSZrLeqAuQJI0/w0KS1MuwkCT1MiwkSb0MC0lSL8NCktTLsNBDJHlnkn8adR2TJflqkoNmaV2/kuTKSdPXJnnBbKy7re+yJHvN1voG2N6BSc6Yq+3NJMnHk7x91HVoOAyLBSjJ7yZZkeSuJDe1P8b/a0S1VJK7Wy0/SXJmkt+Z3Keq9q2qkwZc19Nn6lNV51bVTo+07ra9E5O8e8r6n1VVZ8/G+gdRVZ+qqheu7XJJPtZe87uS/CzJvZOmv7qOtby2qt6zLssm+VaSe5LcmeSO9v48KslGAy6/Qfv5L12X7aufYbHAJDkS+ADwHmAb4MnAR4H9RljWzlW1GbATcCLwd0mOme2NJNlgttc5X1XVYVW1WXvd3wOcPDFdVftO7T9Hr91hVfU4YDvgKOD3gC8nyRxsWz0MiwUkyRbAu4DDq+qLVXV3Vd1bVf9SVX+8hmU+l+RHSW5Pck6SZ02a95Ikl7f/Bm9I8n9b+9ZJvpzktiS3Jjk3Se97rapuqap/BN4AvC3JVm19Zyd5bXv+9CT/r9VzS5KTW/s5bTUXt/+OfyfJXklWJnlrkh8Bn5hom7Lp57bvY3WSTyTZuK3z95N8a8rrUa2GQ4EDgaPa9v6lzX9wWCvJY5J8IMmN7fGBJI9p8yZqe0uSm9se3sF9r+00P5+H1NjqOyzJVe37+ci6/LFt32MlOTjJdcAZSdZL8vn2frit/Vx+cdIy/5Tkne35C9prcVSSVe37f80g266qu6rqLLp/YH4FeFFb5+5JzmvbvinJh5Js2Bab+Plf1n4eL0+yVZLT2/ZXJ/mXJIvX9rVQx7BYWHYHNga+tBbLfBXYEXgicCHwqUnzlgOvb/8NPhs4q7W/BVgJLKLbe3k7sDbXlTkV2ADYbZp5fwGcATweWAJ8GKCq9mzzd27/HZ/cpp8EPAHYATh0Dds7kO4P0tOAXwD+tK/Aqjqe7rV4b9veb0zT7U+A5wO7ADu372fyup8EbAEsBg4BPpLk8W3eml7bQbwMeG7b5m+3721d7Qk8A3hpm/4y3fvhScClwD/OsOwS4LF0ewqHAccl2XzQDVfVD4Dv0gUGwH3AEcDWwB7Ai4HXT6oT4Fnt5/EFur9v/0C397wDcC/wwUG3r4cyLBaWrYBbquq+QReoqhOq6s6q+h/gncDObQ8Ful++ZybZvKpWV9WFk9q3BXZoey7n1lpchKyq7gVuofsjP9W9dL/421XVPVX1rWn6TPYAcExV/U9V/fca+vxdVV1fVbcCfwm8atBaexwIvKuqbq6qVcCfA6+eNP/eNv/eqjoduItuKG5i3nSv7SCOrarbquo64Jt0YbWujqmqn1bVf1fVA1V1Yns/3EP3fvilJJuuYdl7gHe37+804H/ownht3Eh7H1TVd6rq/Kq6r6quAY4HfnVNC1bVqqr6Uqv9DrrhtjX218wMi4XlJ8DWg44/J1k/ybFJ/ivJHcC1bdbW7evLgZcAP2xDQ7u39vcBV9MNXVyT5Oi1KbINLSwCbp1m9lFAgP9Id+bRH/SsblX7wzaT6yc9/yHdf8KzYbu2vjWt+ydTgvunwGbt+Zpe20H8aA3rXBcPvjbt/fDe9jO9g+5nDD9/P0x1S1Xd/whrWUx7HyR5RpKvtGGwO+iGVNe0bZJsmu4Mreta/7Nm6q+ZGRYLy7/T/be3/4D9f5du3PgFdMMlS1t74MH/9PajG6L6Z+CU1n5nVb2lqp4K/AZwZJJ91qLO/eiGHP5j6oyq+lFVva6qtqMbgvhoZj4DapA9mu0nPX8y3X+zAHcDm0zMSPKktVz3jXR7QdOte0Zrem3n2pQ9wtfQBdjedO+Hidd9KAeg053ZtAtwbmv6e7qhr6dX1ebAOyZte7qfxVHAU4DdWv+9h1HnQmFYLCBVdTvdL9hHkuyfZJMkGybZN8l7p1nkcXRDBz+h+6P54GmRSTZKd47/Fm3Y6A7g/jbvZe0AaSa13/+wtU+R5AlJDgQ+AvxVVf1kmj4HJFnSJlfT/ZGYWPePgacO8FJMdXiSJUmeQHd8ZeJ4x8XAs5Ls0g56v3PKcn3b+wzwp0kWJdma7rXv/QzLTK/tiE19P/zlMDbS9gj2ogvJbwNfn7T924G724H1ieMVtD2Yn/DQn8fj6PZmVqc7WeIdw6h3oTAsFpiqej9wJN2B1lV0wwxvpPvFnOqTdEMnNwCXA+dNmf9q4Nq2i38Y3amO0B0A/Ve6Mfh/Bz7a89mDi5PcRTes8VrgzVW1pl/s5wLnt/6nAUe0A6HQ/TE/qZ0t89szbG+qT9MdNL+mPd4NUFXfpxvq+FfgKmDq8ZHldMcVbksy3ev3bmAF8J/AJXQnCLx7mn7TWdNrO0qfoNszuhG4DPi3WV7/x5LcSTeM9n660H7ppL2btwAHAXfS7WWcPGX5Y4BPt5/H/27r2IIuRP6N7mQNraN48yNJUh/3LCRJvQwLSVIvw0KS1MuwkCT1elReWG3rrbeupUuXjroMSZpXLrjggluqatF08x6VYbF06VJWrFgx6jIkaV5J8sM1zXMYSpLUy7CQJPUyLCRJvQwLSVIvw0KS1MuwkCT1MiwkSb0MC0lSL8NCktTrUfkJ7kdq6dFfGXUJD7r22JeOugRJcs9CktTPsJAk9TIsJEm9DAtJUi/DQpLUy7OhJOkRGKezJ2F4Z1C6ZyFJ6mVYSJJ6GRaSpF5DC4skJyS5Ocmlk9qekOQbSa5qXx/f2pPkQ0muTvKfSXadtMxBrf9VSQ4aVr2SpDUb5p7FicCLp7QdDZxZVTsCZ7ZpgH2BHdvjUOA46MIFOAZ4HrAbcMxEwEiS5s7QwqKqzgFundK8H3BSe34SsP+k9k9W5zxgyyTbAi8CvlFVt1bVauAbPDyAJElDNtfHLLapqpsA2tcntvbFwPWT+q1sbWtqlyTNoXE5wJ1p2mqG9oevIDk0yYokK1atWjWrxUnSQjfXYfHjNrxE+3pza18JbD+p3xLgxhnaH6aqjq+qZVW1bNGiRbNeuCQtZHMdFqcBE2c0HQScOqn9Ne2sqOcDt7dhqq8DL0zy+HZg+4WtTZI0h4Z2uY8knwH2ArZOspLurKZjgVOSHAJcBxzQup8OvAS4GvgpcDBAVd2a5C+A77R+76qqqQfNJUlDNrSwqKpXrWHWPtP0LeDwNaznBOCEWSxNkrSWxuUAtyRpjBkWkqReXqJcI7FQLussPVq4ZyFJ6mVYSJJ6GRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF4jCYskb05yWZJLk3wmycZJnpLk/CRXJTk5yUat72Pa9NVt/tJR1CxJC9mch0WSxcD/AZZV1bOB9YFXAn8F/G1V7QisBg5pixwCrK6qpwN/2/pJkubQqIahNgAem2QDYBPgJmBv4PNt/knA/u35fm2aNn+fJJnDWiVpwZvzsKiqG4C/Bq6jC4nbgQuA26rqvtZtJbC4PV8MXN+Wva/132rqepMcmmRFkhWrVq0a7jchSQvMKIahHk+3t/AUYDtgU2DfabrWxCIzzPt5Q9XxVbWsqpYtWrRotsqVJDGaYagXAD+oqlVVdS/wReCXgS3bsBTAEuDG9nwlsD1Am78FcOvclixJC9sowuI64PlJNmnHHvYBLge+Cbyi9TkIOLU9P61N0+afVVUP27OQJA3PKI5ZnE93oPpC4JJWw/HAW4Ejk1xNd0xieVtkObBVaz8SOHqua5akhW6D/i6zr6qOAY6Z0nwNsNs0fe8BDpiLuiRJ0/MT3JKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXmsdFknWS7L5MIqRJI2ngcIiyaeTbJ5kU7obFV2Z5I+HW5okaVwMumfxzKq6A9gfOB14MvDqoVUlSRorg4bFhkk2pAuLU9u9s721qSQtEIOGxd8D1wKbAuck2QG4Y1hFSZLGy0C3Va2qDwEfmtT0wyS/NpySJEnjZtAD3NskWZ7kq236mcBBQ61MkjQ2Bh2GOhH4OrBdm/4+8KZhFCRJGj+DhsXWVXUK8ABAVd0H3D+0qiRJY2XQsLg7yVa0M6CSPB+4fWhVSZLGykAHuIEjgdOApyX5NrAIeMXQqpIkjZVBz4a6MMmvAjsBAa5sn7WQJC0Ag54NdTiwWVVdVlWXApsl+cPhliZJGheDHrN4XVXdNjFRVauB1w2nJEnSuBk0LNZLkomJJOsDGw2nJEnSuBn0APfXgVOSfIzujKjDgK8NrSpJ0lgZNCzeCrweeAPdAe4zgI8PqyhJ0ngZ9GyoB4Dj2uMRS7IlXdg8m25P5Q+AK4GTgaV0Fy387apa3Ya/Pgi8BPgp8PtVdeFs1CFJGsygZ0PtkeQbSb6f5JokP0hyzSPY7geBr1XVM4CdgSuAo4Ezq2pH4Mw2DbAvsGN7HMosBZYkaXCDDkMtB94MXMAjvMxHuyXrnsDvA1TVz4CfJdkP2Kt1Owk4m274az/gk1VVwHlJtkyybVXd9EjqkCQNbtCwuL2qvjpL23wqsAr4RJKd6QLoCGCbiQCoqpuSPLH1XwxcP2n5la3tIWGR5FC6PQ+e/OQnz1KpkiQY/NTZbyZ5X5Ldk+w68VjHbW4A7AocV1XPAe7m50NO08k0bQ+7S19VHV9Vy6pq2aJFi9axNEnSdAbds3he+7psUlsBe6/DNlcCK6vq/Db9ebqw+PHE8FKSbYGbJ/XfftLyS4Ab12G7kqR1NOjZULN2V7yq+lGS65PsVFVXAvsAl7fHQcCx7eupbZHTgDcm+SxdaN3u8QpJmlsDhUWSbYD3ANtV1b7tTnm7V9XyddzuHwGfSrIRcA1wMN2Q2ClJDgGuAw5ofU+nO232arpTZw9ex21KktbRoMNQJwKfAP6kTX+f7jMR6xQWVXURDx3SmrDPNH0LOHxdtiNJmh3eKU+S1Ms75UmSenmnPElSr96wSLIesDHgnfIkaYHqDYuqeiDJ31TV7sBlc1CTJGnMDHrM4owkL598AyRJ0sKxNscsNgXuS3IP3VBUVdXmQ6tMkjQ2Bv0E9+OGXYgkaXwN+gnuPadrr6pzZrccSdI4GnQY6o8nPd8Y2I3u0uLrciFBSdI8M+gw1G9Mnk6yPfDeoVQkSRo7g54NNdVKuvtnS5IWgEGPWXyYn99waD1gF+DiYRUlSRovgx6zWDHp+X3AZ6rq20OoR5I0hgYNi88D91TV/QBJ1k+ySVX9dHilSZLGxaDHLM4EHjtp+rHAv85+OZKkcTRoWGxcVXdNTLTnmwynJEnSuBl0GOruJLtW1YUASX4J+O/hlaW1sfTor4y6hIe49tiXjroESbNs0LB4E/C5JDe26W2B3xlOSZKkcTPoh/K+k+QZ/Px+Ft/zfhbS+BunvU73OOe3gY5ZJDkc2LSqLq2qS4DNkvzhcEuTJI2LQQ9wv66qbpuYqKrVwOuGU5IkadwMGhbrTb7xUZL1gY2GU5IkadwMeoD7DOCUJB+ju+zHG4CvDa0qSdJYGTQs/oxu2OkwugPcZwDLh1WUJGm8zBgWSTYA3gMcDFxPFxTbAz+gG8K6f9gFSpJGr++YxfuAJwBPrapdq+o5wFOALYC/HnZxkqTx0BcWL6M7E+rOiYb2/A3AS4ZZmCRpfPSFRVVVTdN4Pz+/v4Uk6VGuLywuT/KaqY1Jfg/43iPZcLvM+XeTfLlNPyXJ+UmuSnJyko1a+2Pa9NVt/tJHsl1J0trrOxvqcOCLSf4AuIBub+K5dJco/61HuO0jgCuAzdv0XwF/W1WfbafoHgIc176urqqnJ3ll6+d1qTTnxunSGeDlMzS3ZtyzqKobqup5wLuAa4HrgHdV1W5VdcO6bjTJEuClwMfbdIC96W6yBHASsH97vl+bps3fZ/IHBCVJwzfohQTPAs6axe1+ADgKeFyb3gq4rarua9MrgcXt+WK603apqvuS3N763zKL9UiSZjDo5T5mTZKXATdX1QWTm6fpWgPMm7zeQ5OsSLJi1apVs1CpJGnCnIcFsAfwm0muBT5LN/z0AWDL9iFAgCXAxL0zVtJ9EHDiQ4JbALdOXWlVHV9Vy6pq2aJFi4b7HUjSAjPnYVFVb6uqJVW1FHglcFZVHQh8E3hF63YQcGp7flqbps0/a7rTeSVJwzOKPYs1eStwZJKr6Y5JTFx7ajmwVWs/Ejh6RPVJ0oI16IUEh6KqzgbObs+vAXabps89wAFzWpgk6SHGac9CkjSmDAtJUi/DQpLUy7CQJPUyLCRJvQwLSVIvw0KS1MuwkCT1MiwkSb0MC0lSL8NCktTLsJAk9TIsJEm9DAtJUi/DQpLUy7CQJPUyLCRJvQwLSVIvw0KS1MuwkCT1MiwkSb0MC0lSL8NCktTLsJAk9TIsJEm9DAtJUi/DQpLUy7CQJPUyLCRJvQwLSVKvOQ+LJNsn+WaSK5JcluSI1v6EJN9IclX7+vjWniQfSnJ1kv9Msutc1yxJC90o9izuA95SVb8IPB84PMkzgaOBM6tqR+DMNg2wL7BjexwKHDf3JUvSwjbnYVFVN1XVhe35ncAVwGJgP+Ck1u0kYP/2fD/gk9U5D9gyybZzXLYkLWgjPWaRZCnwHOB8YJuqugm6QAGe2LotBq6ftNjK1jZ1XYcmWZFkxapVq4ZZtiQtOCMLiySbAV8A3lRVd8zUdZq2elhD1fFVtayqli1atGi2ypQkMaKwSLIhXVB8qqq+2Jp/PDG81L7e3NpXAttPWnwJcONc1SpJGs3ZUAGWA1dU1fsnzToNOKg9Pwg4dVL7a9pZUc8Hbp8YrpIkzY0NRrDNPYBXA5ckuai1vR04FjglySHAdcABbd7pwEuAq4GfAgfPbbmSpDkPi6r6FtMfhwDYZ5r+BRw+1KIkSTPyE9ySpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqde8CYskL05yZZKrkxw96nokaSGZF2GRZH3gI8C+wDOBVyV55mirkqSFY16EBbAbcHVVXVNVPwM+C+w34pokacFIVY26hl5JXgG8uKpe26ZfDTyvqt44qc+hwKFtcifgyjkv9KG2Bm4ZcQ1ry5rnxnyreb7VC9a8rnaoqkXTzdhgritZR5mm7SEpV1XHA8fPTTn9kqyoqmWjrmNtWPPcmG81z7d6wZqHYb4MQ60Etp80vQS4cUS1SNKCM1/C4jvAjkmekmQj4JXAaSOuSZIWjHkxDFVV9yV5I/B1YH3ghKq6bMRl9RmbIbG1YM1zY77VPN/qBWuedfPiALckabTmyzCUJGmEDAtJUi/DYpYlOSHJzUkuHXUtg0qyfZJvJrkiyWVJjhh1TX2SbJzkP5Jc3Gr+81HXNIgk6yf5bpIvj7qWQSS5NsklSS5KsmLU9QwiyZZJPp/ke+09vfuoa5pJkp3a6zvxuCPJm0Zd11Qes5hlSfYE7gI+WVXPHnU9g0iyLbBtVV2Y5HHABcD+VXX5iEtboyQBNq2qu5JsCHwLOKKqzhtxaTNKciSwDNi8ql426nr6JLkWWFZVo/6w2MCSnAScW1Ufb2dPblJVt426rkG0SxvdQPeh4x+Oup7J3LOYZVV1DnDrqOtYG1V1U1Vd2J7fCVwBLB5tVTOrzl1tcsP2GOv/fJIsAV4KfHzUtTxaJdkc2BNYDlBVP5svQdHsA/zXuAUFGBaaIslS4DnA+aOtpF8b0rkIuBn4RlWNe80fAI4CHhh1IWuhgDOSXNAuqTPungqsAj7Rhvs+nmTTURe1Fl4JfGbURUzHsNCDkmwGfAF4U1XdMep6+lTV/VW1C90n+ndLMrbDfkleBtxcVReMupa1tEdV7Up3xefD2zDrONsA2BU4rqqeA9wNzItbGrQhs98EPjfqWqZjWAiANu7/BeBTVfXFUdezNtoww9nAi0dcykz2AH6zHQP4LLB3kn8abUn9qurG9vVm4Et0V4AeZyuBlZP2Mj9PFx7zwb7AhVX141EXMh3DQhMHi5cDV1TV+0ddzyCSLEqyZXv+WOAFwPdGW9WaVdXbqmpJVS2lG2o4q6p+b8RlzSjJpu2EB9pQzguBsT7Lr6p+BFyfZKfWtA8wtidqTPEqxnQICubJ5T7mkySfAfYCtk6yEjimqpaPtqpeewCvBi5pxwAA3l5Vp4+wpj7bAie1s0fWA06pqnlxOuo8sg3wpe5/CTYAPl1VXxttSQP5I+BTbVjnGuDgEdfTK8kmwK8Drx91LWviqbOSpF4OQ0mSehkWkqRehoUkqZdhIUnqZVhIknoZFtKAkjwpyWeT/FeSy5OcnuQX5tMVhqV15ecspAG0Dy5+CTipql7Z2nah+yyC9KjnnoU0mF8D7q2qj000VNVFwPUT00mWJjk3yYXt8cutfdsk57R7FVya5FfaRRBPbNOXJHlz6/u0JF9rF+47N8kzWvsBre/FSc6Z229dcs9CGtSz6e7zMZObgV+vqnuS7Eh36YZlwO8CX6+qv2yfON8E2AVYPHHPk4lLlwDHA4dV1VVJngd8FNgbeAfwoqq6YVJfac4YFtLs2RD4uzY8dT/wC639O8AJ7WKN/1xVFyW5Bnhqkg8DX6G7DPhmwC8Dn2uX2AB4TPv6beDEJKcA8+pCj3p0cBhKGsxlwC/19Hkz8GNgZ7o9io3gwRti7Ul3B7R/TPKaqlrd+p0NHE53Q6T1gNuqapdJj19s6zgM+FNge+CiJFvN8vcnzciwkAZzFvCYJK+baEjyXGCHSX22AG6qqgfoLsy4fuu3A929LP6B7uq+uybZGlivqr4A/Bmwa7uHyA+SHNCWS5Kd2/OnVdX5VfUO4Ba60JDmjGEhDaC6K27+FvDr7dTZy4B3AjdO6vZR4KAk59ENQd3d2vei2xv4LvBy4IN0t609u13l90Tgba3vgcAhSS6m25vZr7W/rx0IvxQ4B7h4GN+ntCZedVaS1Ms9C0lSL8NCktTLsJAk9TIsJEm9DAtJUi/DQpLUy7CQJPX6//syQlzSEnutAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alldata = pd.read_csv(\"sat.all.csv\")\n",
    "traindata = alldata[alldata['TrainTest'] == 'train']\n",
    "testdata = alldata[alldata['TrainTest'] == 'test']\n",
    "\n",
    "distribution = traindata.groupby('Class').count()\n",
    "plt.bar(distribution.index.values,distribution.iloc[:,0])\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Occurences')\n",
    "plt.title('Class Distributions in Train Data')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "train, validate = train_test_split(traindata, test_size=0.2, random_state=seed, stratify=traindata['Class'], shuffle=True)\n",
    "\n",
    "trainX = traindata.iloc[:,0:36].to_numpy()\n",
    "trainY= traindata.iloc[:,36].to_numpy()\n",
    "\n",
    "testX = testdata.iloc[:,0:36].to_numpy()\n",
    "testY = testdata.iloc[:,36].to_numpy()\n",
    "\n",
    "for i in range(1,8):\n",
    "    trainY[trainY==i] = i-1\n",
    "    testY[testY==i] = i-1\n",
    "\n",
    "trainY[trainY==6] = 5\n",
    "testY[testY==6] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scalerFunc = scaler.fit(trainX)\n",
    "\n",
    "trainX = scalerFunc.transform(trainX)\n",
    "testX = scalerFunc.transform(testX)\n",
    "\n",
    "testX_tensor = torch.from_numpy(testX).float()\n",
    "testY_tensor = torch.from_numpy(testY).float()\n",
    "\n",
    "#for i in range(0,36):\n",
    "#    print(' Feature {} \\t Train: {:.6f} {:.6f} Validation: {:.6f} {:.6f} Test: {:.6f} {:.6f}'.format(\n",
    "#          i,\n",
    "#          min(trainX[:,i]),max(trainX[:,i]),\n",
    "#          min(valX[:,i]),max(valX[:,i]),\n",
    "#          min(testX[:,i]),max(testX[:,i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = 36\n",
    "out_classes = 6\n",
    "\n",
    "class TestModel(Module):\n",
    "    def __init__(self, num_units=in_features*2, hidden_layers=4 ):\n",
    "        \n",
    "        self.hidden_layers = hidden_layers\n",
    "        hidden = []\n",
    "        hidden.append(num_units)\n",
    "        for i in range(1,hidden_layers):\n",
    "            hidden.append(max(out_classes*2,int(num_units/2**i)))\n",
    "        \n",
    "        super(TestModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden[0])\n",
    "\n",
    "        if hidden_layers > 1:\n",
    "            self.relu1 = nn.ReLU()\n",
    "            self.fc2 = nn.Linear(hidden[0], hidden[1])\n",
    "        if hidden_layers > 2:\n",
    "            self.relu2 = nn.ReLU()\n",
    "            self.fc3 = nn.Linear(hidden[1], hidden[2])\n",
    "        if hidden_layers > 3:\n",
    "            self.relu3 = nn.ReLU()\n",
    "            self.fc4 = nn.Linear(hidden[2], hidden[3])\n",
    "        if hidden_layers > 4:\n",
    "            self.relu4 = nn.ReLU()\n",
    "            self.fc5 = nn.Linear(hidden[3], hidden[4])\n",
    "        if hidden_layers > 5:\n",
    "            self.relu5 = nn.ReLU()\n",
    "            self.fc6 = nn.Linear(hidden[4], hidden[5])\n",
    "        \n",
    "        self.reluLast = nn.ReLU()\n",
    "        self.fcLast = nn.Linear(hidden[hidden_layers-1], out_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        hidden_layers = self.hidden_layers\n",
    "        \n",
    "        y = self.fc1(x)\n",
    "\n",
    "        if hidden_layers > 1:\n",
    "            y = self.relu1(y)\n",
    "            y = self.fc2(y)\n",
    "        if hidden_layers > 2:\n",
    "            y = self.relu2(y)\n",
    "            y = self.fc3(y)\n",
    "        if hidden_layers > 3:\n",
    "            y = self.relu3(y)\n",
    "            y = self.fc4(y)\n",
    "        if hidden_layers > 4:\n",
    "            y = self.relu4(y)\n",
    "            y = self.fc5(y)\n",
    "        if hidden_layers > 5:\n",
    "            y = self.relu5(y)\n",
    "            y = self.fc6(y)\n",
    "\n",
    "        y = self.reluLast(y)\n",
    "        y = self.fcLast(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunModel(model, epochs, trainX, trainY, valX, valY):\n",
    "    accuracies = []\n",
    "    validations = []\n",
    "    errors = []\n",
    "\n",
    "    train_score = 0\n",
    "    epoch = 0\n",
    "\n",
    "    trainXY_tensor = TensorDataset(trainX, trainY)\n",
    "    train_loader = DataLoader(trainXY_tensor, batch_size=batch_size)\n",
    "        \n",
    "    while (epoch < epochs):\n",
    "        batch = 0\n",
    "        for idx, (train_x, train_label) in enumerate(train_loader):\n",
    "            train_x = train_x.to(device)\n",
    "            train_label = train_label.to(device)\n",
    "            opt.zero_grad()\n",
    "            predict_y = model(train_x)\n",
    "            error = loss(predict_y, train_label.long())\n",
    "            error.backward()\n",
    "            opt.step()\n",
    "\n",
    "            batch = batch + 1\n",
    "    \n",
    "\n",
    "        train_pred, train_score = Check(trainX, trainY, model)\n",
    "        error = loss(model(trainX), trainY.to(device).long())\n",
    "        val_pred, val_score = Check(valX, valY, model)\n",
    "\n",
    "        accuracies.append(train_score)\n",
    "        validations.append(val_score)\n",
    "        errors.append(error)\n",
    "    \n",
    "        epoch += 1\n",
    "        \n",
    "    return(accuracies, errors, validations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunFolds(nfolds, trainX, trainY, outputrequired):\n",
    "    \n",
    "    fold = 0\n",
    "    torch.save(model.state_dict(),'initial_weights.state')\n",
    "\n",
    "    foldAcc = []\n",
    "    foldVal =  []\n",
    "    foldErr = []\n",
    "\n",
    "    for trainIndex, testIndex in nfolds.split(trainX):\n",
    "    \n",
    "        model.load_state_dict(torch.load('initial_weights.state'))\n",
    "\n",
    "        trainX_tensor = torch.from_numpy(trainX[trainIndex]).float()\n",
    "        trainY_tensor = torch.from_numpy(trainY[trainIndex]).float()\n",
    "\n",
    "        valX_tensor = torch.from_numpy(trainX[testIndex]).float()\n",
    "        valY_tensor = torch.from_numpy(trainY[testIndex]).float()\n",
    "\n",
    "        time00 = time.perf_counter()\n",
    "        torch.manual_seed(7)\n",
    "\n",
    "        acc, val, err = RunModel(model, epochs, trainX_tensor, trainY_tensor, valX_tensor, valY_tensor)\n",
    "    \n",
    "        time01 = time.perf_counter()\n",
    "        train_pred, train_score = Check(trainX_tensor, trainY_tensor, model)\n",
    "        error = loss(model(trainX_tensor), trainY_tensor.to(device).long()).detach().cpu().numpy()\n",
    "        val_pred, val_score = Check(valX_tensor, valY_tensor, model)\n",
    "        if outputrequired:\n",
    "            print('fold: {} \\t accuracy: {:.6f} error: {:.6f} validation accuracy: {:.6f} time {:.3f}s'.format(\n",
    "               fold + 1, train_score, error, val_score, time01 - time00))\n",
    "\n",
    "        foldAcc.append(train_score)\n",
    "        foldVal.append(val_score)\n",
    "        foldErr.append(error)\n",
    "    \n",
    "        fold += 1\n",
    "\n",
    "    return(foldAcc, foldVal, foldErr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "indir = './DataForRuns/'\n",
    "infile = 'NumUnits2Adam'\n",
    "allruns = pd.read_csv(indir + infile + \".csv\",sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[200.0, 2.0, 512.0, 0.01, 100.0, 10.0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allruns.T[0].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing run number 0\n",
      "Using 2 hidden_layers with 200 units in first layer. Batch size of 512 for 100 epochs. 10 folds. Learning rate 1.0e-02\n",
      "\n",
      "Results Accuracy: mean 0.924 std 0.020 Validation Accuracy: mean 0.892 std 0.023 Runtime 20.57 seconds\n",
      "\n",
      "\n",
      "Executing run number 1\n",
      "Using 2 hidden_layers with 150 units in first layer. Batch size of 512 for 100 epochs. 10 folds. Learning rate 1.0e-02\n",
      "\n",
      "Results Accuracy: mean 0.932 std 0.006 Validation Accuracy: mean 0.900 std 0.015 Runtime 19.06 seconds\n",
      "\n",
      "\n",
      "Executing run number 2\n",
      "Using 2 hidden_layers with 100 units in first layer. Batch size of 512 for 100 epochs. 10 folds. Learning rate 1.0e-02\n",
      "\n",
      "Results Accuracy: mean 0.929 std 0.007 Validation Accuracy: mean 0.891 std 0.011 Runtime 19.09 seconds\n",
      "\n",
      "\n",
      "Executing run number 3\n",
      "Using 2 hidden_layers with 80 units in first layer. Batch size of 512 for 100 epochs. 10 folds. Learning rate 1.0e-02\n",
      "\n",
      "Results Accuracy: mean 0.927 std 0.005 Validation Accuracy: mean 0.893 std 0.017 Runtime 19.20 seconds\n",
      "\n",
      "\n",
      "Executing run number 4\n",
      "Using 2 hidden_layers with 60 units in first layer. Batch size of 512 for 100 epochs. 10 folds. Learning rate 1.0e-02\n",
      "\n",
      "Results Accuracy: mean 0.919 std 0.005 Validation Accuracy: mean 0.894 std 0.013 Runtime 19.00 seconds\n",
      "\n",
      "\n",
      "Executing run number 5\n",
      "Using 2 hidden_layers with 40 units in first layer. Batch size of 512 for 100 epochs. 10 folds. Learning rate 1.0e-02\n",
      "\n",
      "Results Accuracy: mean 0.917 std 0.008 Validation Accuracy: mean 0.894 std 0.010 Runtime 19.04 seconds\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "allresults = []\n",
    "\n",
    "for index,row in allruns.iterrows():\n",
    "    num_units = int(row['num_units'])\n",
    "    hidden_layers = int(row['hidden_layers'])\n",
    "    batch_size = int(row['batch_size'])\n",
    "    lr = row['lr']\n",
    "    epochs = int(row['epochs'])\n",
    "    fold_splits = int(row['fold_splits'])\n",
    "    \n",
    "    blurb = False\n",
    "    \n",
    "    print(\"Executing run number\", count)\n",
    "    print('Using {} hidden_layers with {} units in first layer. Batch size of {} for {} epochs. \\\n",
    "{} folds. Learning rate {:.1e}'.format(\n",
    "            hidden_layers, num_units, batch_size, epochs, fold_splits, lr))\n",
    "    print()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = torch.nn.DataParallel(TestModel(num_units=num_units, hidden_layers=hidden_layers)).cuda()\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        model = TestModel()\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    if blurb:\n",
    "        print(model)\n",
    "\n",
    "    optName = \"Adam\"\n",
    "\n",
    "    if optName == \"Wame\":\n",
    "        opt = wame(model.parameters(), lr=lr)\n",
    "    if optName == \"SGD\":\n",
    "        opt = SGD(model.parameters(), lr=lr)\n",
    "    if optName == \"Adam\":\n",
    "        opt = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    loss = CrossEntropyLoss().to(device)\n",
    "    epochs = 50\n",
    "\n",
    "    nfolds = KFold(n_splits=fold_splits, shuffle=True)\n",
    "    \n",
    "    starttime = time.perf_counter()\n",
    "    foldAcc, foldVal, foldErr = RunFolds(nfolds, trainX, trainY, blurb)\n",
    "    runtime= time.perf_counter() - starttime\n",
    "    \n",
    "    print('Results Accuracy: mean {:.3f} std {:.3f} Validation Accuracy: mean {:.3f} std {:.3f} \\\n",
    "Runtime {:.2f} seconds'.format(\n",
    "        np.mean(foldAcc), np.std(foldAcc), np.mean(foldVal), np.std(foldVal), runtime) )\n",
    "    print()\n",
    "    print()      \n",
    "    \n",
    "    allresults.append(allruns.T[count].to_list() + foldAcc + foldVal + [runtime])\n",
    "          \n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(indir + infile + 'Results.csv', 'w+') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows( allresults )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('initial_weights.state'))\n",
    "\n",
    "trainX_tensor = torch.from_numpy(trainX).float()\n",
    "trainY_tensor = torch.from_numpy(trainY).float()\n",
    "\n",
    "valX_tensor = torch.from_numpy(testX).float()\n",
    "valY_tensor = torch.from_numpy(testY).float()\n",
    "\n",
    "time00 = time.perf_counter()\n",
    "torch.manual_seed(7)\n",
    "\n",
    "trainXY_tensor = TensorDataset(trainX_tensor, trainY_tensor)\n",
    "train_loader = DataLoader(trainXY_tensor, batch_size=batch_size)\n",
    "\n",
    "acc, val, err = RunModel(model, epochs, trainX_tensor, trainY_tensor, valX_tensor, valY_tensor)\n",
    "    \n",
    "time01 = time.perf_counter()\n",
    "train_pred, train_score = Check(trainX_tensor, trainY_tensor, model)\n",
    "error = loss(model(trainX_tensor), trainY_tensor.to(device).long())\n",
    "val_pred, val_score = Check(valX_tensor, valY_tensor, model)\n",
    "\n",
    "print('Overall: \\t accuracy: {:.6f} error: {:.6f} validation accuracy: {:.6f} time {:.3f}s'.format(\n",
    "        train_score, error, val_score, time01 - time00))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotrange = epochs\n",
    "\n",
    "plt.plot(range(plotrange), acc[0:plotrange], color='green')\n",
    "plt.plot(range(plotrange), val[0:plotrange], color='blue')\n",
    "plt.plot(range(plotrange), err[0:plotrange], color='r')\n",
    "title = \"Optimizer {} reached accuracy {:.3f} in {:.2f} seconds\".format(optName, train_score, time01 - time00)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy, Validation and Error')\n",
    "plt.title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PrintResults(val_pred, valY_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
