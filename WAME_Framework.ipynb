{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning- COIY065H7\n",
    "\n",
    "## Coursework, WAME Optimiser Evaluation\n",
    "\n",
    "Copy of this has been loaded to https://github.com/bfromson/ML-WAME-Coursework. It works with runtime GPU in Colab but requires file locations for loading CSV files to be altered and cannot write the result files when running on Colab. If cloning and running locally then the Landsat csv files should be in same directory as the code and the parameter files for controlling runs should be in a directory called \"DataForRuns\" in the same location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load modules for use throughout the workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "indir = './DataForRuns/'\n",
    "infile = 'AdamOnly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import Module\n",
    "from torch.optim import *\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchsummary import summary\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import time\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify versions of ML libraries being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:         3.7.3 (default, Mar 27 2019, 22:11:17) \t[GCC 7.3.0]\n",
      "PyTorch version:        1.5.0\n",
      "CUDA Version:           10.2\n",
      "Scikit-learn Version:   0.22.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python version:        \", sys.version.replace(\"\\n\",\"\\t\"))\n",
    "print(\"PyTorch version:       \", torch.__version__)\n",
    "print(\"CUDA Version:          \", torch.version.cuda)\n",
    "print(\"Scikit-learn Version:  \", sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routines to be used \n",
    "\n",
    "PrintResults will output a table of Precision and Recall for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrintResults(predictions, actuals, printit):\n",
    "    ynp = predictions.numpy()\n",
    "    testnp = actuals.numpy()\n",
    "    if printit:\n",
    "        print(\"Class \\t Cases \\t TP \\t FP \\t Precision \\t Recall \\t F1\")\n",
    "    allcases = 0\n",
    "    allTP = 0\n",
    "    allFP = 0\n",
    "    output=[]\n",
    "    epsilon = 1e-10\n",
    "    F1avg = 0\n",
    "    Pavg = 0\n",
    "    Ravg = 0\n",
    "    for i in range(0,6):\n",
    "        cases = sum(testnp==i)\n",
    "        allcases += cases\n",
    "        TP = sum((ynp==i) & (testnp==i))\n",
    "        FP = sum((ynp==i) & (testnp!=i))\n",
    "        allTP += TP\n",
    "        allFP += FP\n",
    "        Precision = TP / (TP + FP + epsilon)\n",
    "        Recall = TP / cases\n",
    "        F1 = 2 * (Precision * Recall) / (Precision + Recall + epsilon)\n",
    "        Pavg += Precision\n",
    "        Ravg += Recall\n",
    "        F1avg += F1\n",
    "        if printit:\n",
    "            print(i, \"\\t\", cases, \"\\t\" ,TP, \"\\t\", FP, \"\\t\", round(Precision, 3), \"\\t\", \"\\t\", round(Recall, 3), \"\\t\", \"\\t\", round(F1, 3))\n",
    "        output.append(i)\n",
    "        output.append(cases)\n",
    "        output.append(TP)\n",
    "        output.append(FP)\n",
    "    Precision = Pavg / 6\n",
    "    Recall = Ravg / 6\n",
    "    F1 = F1avg / 6\n",
    "    output.append(Precision)\n",
    "    output.append(Recall)\n",
    "    output.append(F1)\n",
    "    if printit:\n",
    "        print(\"Total\", \"\\t\", allcases, \"\\t\" ,allTP, \"\\t\", allFP, \"\\t\", round(Precision, 3), \"\\t\", \"\\t\", round(Recall, 3), \"\\t\", \"\\t\", round(F1,3))\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check returns the class predictions and accuracy score of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Check(Xvals, Yactuals, model):\n",
    "    predictraw = model(Xvals.to(device)).detach().cpu()\n",
    "    predict = np.argmax(predictraw, axis=-1)\n",
    "    predict_score = predict == Yactuals\n",
    "    correct = np.sum(predict_score.numpy(), axis=-1)\n",
    "    sumtot = predict_score.shape[0]\n",
    "    score = correct / sumtot\n",
    "    return (predict, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wame optimizer\n",
    "\n",
    "Logic for the optimiser follows the paper by Mosca, \n",
    "\n",
    "A. Mosca and G. D. Magoulas, ‘Training convolutional networks with weight-wise adaptive learning rates’, in ESANN, 2017.\n",
    "\n",
    "The code for the PyTorch optimizer implementation was adapted from the PyTorch souce code using -\n",
    "\n",
    "PyTorch optimizer class code: https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "Example of RProp implementation: https://pytorch.org/docs/stable/_modules/torch/optim/rprop.html\n",
    "\n",
    "And guidance on writing a custom optimizer for AdamW: http://mcneela.github.io/machine_learning/2019/09/03/Writing-Your-Own-Optimizers-In-Pytorch.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wame(Optimizer):\n",
    "\n",
    "\n",
    "    def __init__(self, params,lr=1e-3, alpha=0.9, etas=(0.1, 1.2), zetas=(0.01, 100), epsilon=1e-10, debug=False ):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= alpha:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(alpha))\n",
    "        if not 0.0 < etas[0] < 1.0 < etas[1]:\n",
    "            raise ValueError(\"Invalid eta values: {}, {}\".format(etas[0], etas[1]))\n",
    "\n",
    "        defaults = dict(alpha=alpha, etas=etas, zetas=zetas, lr=lr, epsilon=epsilon, debug=debug)\n",
    "        super(wame, self).__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "   \n",
    "        for group in self.param_groups:    \n",
    "            etaminus, etaplus = group['etas']\n",
    "            zeta_min, zeta_max = group['zetas']\n",
    "            alpha = group['alpha']\n",
    "            lr = group['lr']\n",
    "            epsilon = group['epsilon']\n",
    "            debug = group['debug']\n",
    "            _p = 0\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('wame does not support sparse gradients')\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['prev'] = torch.ones_like(p.data, memory_format=torch.preserve_format)\n",
    "                    state['prev'] = grad\n",
    "                    state['Theta'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n",
    "                    state['Z'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n",
    "                    state['zeta'] = torch.ones_like(p.data, memory_format=torch.preserve_format)\n",
    "                    state['gradmult'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n",
    "                    \n",
    "                    state['step_size'] = grad.new().resize_as_(grad).fill_(group['lr']) ### check on initialisation\n",
    "                    state['prev'] = grad\n",
    "                    \n",
    "                if (_p == 0) & debug:\n",
    "                    print(\"Weights at start: \",p.data.sum(), p.data.size())\n",
    "                step_size = state['step_size']\n",
    "                Theta = state['Theta']\n",
    "                Z = state['Z']\n",
    "                zeta = state['zeta']\n",
    "                gradmult = state['gradmult']                    \n",
    "                \n",
    "                state['step'] += 1\n",
    "\n",
    "                gradmult = grad.mul(state['prev'])\n",
    "                zeta[gradmult.gt(0.)] = zeta[gradmult.gt(0.)].mul(etaplus).clamp(zeta_min, zeta_max)\n",
    "                zeta[gradmult.lt(0.)] = zeta[gradmult.lt(0.)].mul(etaminus).clamp(zeta_min, zeta_max)\n",
    "                zeta[gradmult.eq(0.)] = 1\n",
    "                \n",
    "                Z = Z.mul(alpha).add(zeta.mul(1 - alpha))\n",
    "                Theta = Theta.mul(alpha).add(grad.mul(grad).mul(1 - alpha))\n",
    "\n",
    "                step_size = Z.mul(-lr).mul(grad).div(Theta.add(epsilon))\n",
    "\n",
    "                grad = grad.clone(memory_format=torch.preserve_format)\n",
    "                \n",
    "                # update parameters\n",
    "                p.data = p.data.add(step_size)\n",
    "                \n",
    "                if (_p == 0) & debug:\n",
    "                    print(\"Step number \", state['step'])\n",
    "                    print(\"Weights after update: \",p.data.sum(), p.data.size())\n",
    "                    print(\"Updates applied: \",step_size.sum(), step_size.size())\n",
    "                    print(\"Theta: \",Theta.sum())\n",
    "                    print(\"Z: \",Z.sum())\n",
    "                    print(\"zeta: \",zeta.sum())\n",
    "                    print(\"grad: \",grad.sum())\n",
    "                    print(\"grad_sqd\", grad_sqd.sum())\n",
    "                    print(\"gradmult\", gradmult.sum())\n",
    "                state['prev'] = grad\n",
    "                \n",
    "                _p = _p + 1\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "The Landsat data recommended in the coursework description has been used for this project. The two files of training and testing data have been merged into a single file with headers added. The features have been labelled as \"F1-F36\" and the class labelled as \"Class\". An additional column named \"TrainTest\" has been added containing the value \"Train\" or \"Test\" to indicate the origin of that row of data. \n",
    "\n",
    "## Load all data from single file\n",
    "\n",
    "Split into train and test datasets. \n",
    "\n",
    "From each dataset extract the features to an X variable and the classes to a Y variable.\n",
    "\n",
    "As the classes are numeric rather than categorical change their values to be sequential from 0 to 5 to simplify the model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = pd.read_csv(\"sat.all.csv\")\n",
    "traindata = alldata[alldata['TrainTest'] == 'train']\n",
    "testdata = alldata[alldata['TrainTest'] == 'test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = traindata.iloc[:,0:36].to_numpy()\n",
    "trainY= traindata.iloc[:,36].to_numpy()\n",
    "\n",
    "testX = testdata.iloc[:,0:36].to_numpy()\n",
    "testY = testdata.iloc[:,36].to_numpy()\n",
    "\n",
    "for i in range(1,8):\n",
    "    trainY[trainY==i] = i-1\n",
    "    testY[testY==i] = i-1\n",
    "\n",
    "trainY[trainY==6] = 5\n",
    "testY[testY==6] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization\n",
    "\n",
    "This routine will be called to normalize data within each fold as the model is built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeData(trainX, valX):\n",
    "    scaler = StandardScaler()\n",
    "    scalerFunc = scaler.fit(trainX)\n",
    "\n",
    "    trainX_norm = scalerFunc.transform(trainX)\n",
    "    valX_norm = scalerFunc.transform(valX)\n",
    "    \n",
    "    return(trainX_norm, valX_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Model Specification\n",
    "\n",
    "This is a parameter driven model where the number of units in the first hidden layer and the number of layers to be used can be specified when a model is instantiated.\n",
    "\n",
    "The ratio of units between consecutive hidden layers has been fixed at 2:1, though this could be altered. \n",
    "\n",
    "An alternative approach would be to allow an array of unit counts to be passed as parameters to the model where each value would represent the number of units in a layer and the number of layers would be the no of items in the array. Thsi approach was not used as it would complicate the automation of the model runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = 36\n",
    "out_classes = 6\n",
    "\n",
    "class TestModel(Module):\n",
    "    def __init__(self, num_units=in_features*2, hidden_layers=4 ):\n",
    "        \n",
    "        self.hidden_layers = hidden_layers\n",
    "        hidden = []\n",
    "        hidden.append(num_units)\n",
    "        for i in range(1,hidden_layers):\n",
    "            hidden.append(max(out_classes*2,int(num_units/2**i)))\n",
    "        \n",
    "        super(TestModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden[0])\n",
    "\n",
    "        if hidden_layers > 1:\n",
    "            self.relu1 = nn.ReLU()\n",
    "            self.fc2 = nn.Linear(hidden[0], hidden[1])\n",
    "        if hidden_layers > 2:\n",
    "            self.relu2 = nn.ReLU()\n",
    "            self.fc3 = nn.Linear(hidden[1], hidden[2])\n",
    "        if hidden_layers > 3:\n",
    "            self.relu3 = nn.ReLU()\n",
    "            self.fc4 = nn.Linear(hidden[2], hidden[3])\n",
    "        if hidden_layers > 4:\n",
    "            self.relu4 = nn.ReLU()\n",
    "            self.fc5 = nn.Linear(hidden[3], hidden[4])\n",
    "        if hidden_layers > 5:\n",
    "            self.relu5 = nn.ReLU()\n",
    "            self.fc6 = nn.Linear(hidden[4], hidden[5])\n",
    "        \n",
    "        self.reluLast = nn.ReLU()\n",
    "        self.fcLast = nn.Linear(hidden[hidden_layers-1], out_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        hidden_layers = self.hidden_layers\n",
    "        \n",
    "        y = self.fc1(x)\n",
    "\n",
    "        if hidden_layers > 1:\n",
    "            y = self.relu1(y)\n",
    "            y = self.fc2(y)\n",
    "        if hidden_layers > 2:\n",
    "            y = self.relu2(y)\n",
    "            y = self.fc3(y)\n",
    "        if hidden_layers > 3:\n",
    "            y = self.relu3(y)\n",
    "            y = self.fc4(y)\n",
    "        if hidden_layers > 4:\n",
    "            y = self.relu4(y)\n",
    "            y = self.fc5(y)\n",
    "        if hidden_layers > 5:\n",
    "            y = self.relu5(y)\n",
    "            y = self.fc6(y)\n",
    "\n",
    "        y = self.reluLast(y)\n",
    "        y = self.fcLast(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunModel\n",
    "\n",
    "Executes a single model run with inputs of the model to run and training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunModel(model, opt, epochs, batch_size, trainX, trainY, valX, valY):\n",
    "    accuracies = []\n",
    "    validations = []\n",
    "    errors = []\n",
    "\n",
    "    train_score = 0\n",
    "    epoch = 0\n",
    "\n",
    "    trainXY_tensor = TensorDataset(trainX, trainY)\n",
    "    train_loader = DataLoader(trainXY_tensor, batch_size=batch_size)\n",
    "        \n",
    "    while (epoch < epochs):\n",
    "        batch = 0\n",
    "        for idx, (train_x, train_label) in enumerate(train_loader):\n",
    "            train_x = train_x.to(device)\n",
    "            train_label = train_label.to(device)\n",
    "            opt.zero_grad()\n",
    "            predict_y = model(train_x)\n",
    "            error = loss(predict_y, train_label.long())\n",
    "            error.backward()\n",
    "            opt.step()\n",
    "\n",
    "            batch = batch + 1\n",
    "    \n",
    "\n",
    "        train_pred, train_score = Check(trainX, trainY, model)\n",
    "        error = loss(model(trainX), trainY.to(device).long())\n",
    "        val_pred, val_score = Check(valX, valY, model)\n",
    "\n",
    "        accuracies.append(train_score)\n",
    "        validations.append(val_score)\n",
    "        errors.append(error)\n",
    "    \n",
    "        epoch += 1   \n",
    "        \n",
    "    return(accuracies, errors, validations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunFolds\n",
    "\n",
    "Runs the model using the supplied feature and class datasets with the data split into the number of folds requested. The final parameter is a boolean which if instructs the routine to print progress information on each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunFolds(model, opt, epochs, nfolds, batch_size, trainX, trainY, outputrequired):\n",
    "    \n",
    "    fold = 0\n",
    "    torch.save(model.state_dict(),'initial_weights.state')\n",
    "\n",
    "    foldAcc = []\n",
    "    foldVal =  []\n",
    "    foldErr = []\n",
    "    allResults = []\n",
    "\n",
    "    for trainIndex, testIndex in nfolds.split(trainX, trainY):\n",
    "        \n",
    "        foldTrainX = trainX[trainIndex]\n",
    "        foldValX = trainX[testIndex]\n",
    "        \n",
    "        foldTrainY = trainY[trainIndex]\n",
    "        foldValY = trainY[testIndex]\n",
    "        \n",
    "        if outputrequired:\n",
    "            classcount = []\n",
    "            for i in range(0,6):\n",
    "                classcount.append(sum(foldTrainY==i))\n",
    "            print(\"Before processing, item count and distribution\")\n",
    "            print(\"Fold \",fold + 1,\" before processing, item count and distribution \",sum(classcount),classcount,    \n",
    "                  \" X range: \",foldTrainX.min(),foldTrainX.max())\n",
    "\n",
    "        model.load_state_dict(torch.load('initial_weights.state'))\n",
    "    \n",
    "        foldTrainX, foldValX = normalizeData(foldTrainX, foldValX)\n",
    "\n",
    "        #oversampler = RandomOverSampler(sampling_strategy='minority')\n",
    "        #oversampler = RandomOverSampler()\n",
    "        #foldTrainX, foldTrainY = oversampler.fit_resample(foldTrainX, foldTrainY)\n",
    "        #foldTrainX, foldTrainY = oversampler.fit_resample(foldTrainX, foldTrainY)\n",
    "        #foldTrainX, foldTrainY = oversampler.fit_resample(foldTrainX, foldTrainY)\n",
    "        \n",
    "        if outputrequired:\n",
    "            classcount = []\n",
    "            for i in range(0,6):\n",
    "                classcount.append(sum(foldTrainY==i))\n",
    "            print(\"Fold \",fold + 1,\"  after processing, item count and distribution \",sum(classcount),classcount,    \n",
    "                  \" X range: {:.3f} {:.3f}\".format(foldTrainX.min(),foldTrainX.max()))\n",
    "            \n",
    "\n",
    "        trainX_tensor = torch.from_numpy(foldTrainX).float()\n",
    "        trainY_tensor = torch.from_numpy(foldTrainY).float()\n",
    "\n",
    "        valX_tensor = torch.from_numpy(foldValX).float()\n",
    "        valY_tensor = torch.from_numpy(foldValY).float()\n",
    "\n",
    "        time00 = time.perf_counter()\n",
    "        torch.manual_seed(7)\n",
    "\n",
    "        acc, val, err = RunModel(model, opt, epochs, batch_size, trainX_tensor, trainY_tensor, valX_tensor, valY_tensor)\n",
    "    \n",
    "        time01 = time.perf_counter()\n",
    "        train_pred, train_score = Check(trainX_tensor, trainY_tensor, model)\n",
    "        error = loss(model(trainX_tensor), trainY_tensor.to(device).long()).detach().cpu().numpy()\n",
    "        val_pred, val_score = Check(valX_tensor, valY_tensor, model)\n",
    "        if outputrequired:\n",
    "            print('fold: {} \\t accuracy: {:.6f} error: {:.6f} validation accuracy: {:.6f} time {:.3f}s'.format(\n",
    "               fold + 1, train_score, error, val_score, time01 - time00))\n",
    "        allResults.extend(PrintResults(val_pred, valY_tensor, outputrequired))\n",
    "\n",
    "        foldAcc.append(train_score)\n",
    "        foldVal.append(val_score)\n",
    "        foldErr.append(error)\n",
    "    \n",
    "        fold += 1\n",
    "\n",
    "    return(foldAcc, foldVal, foldErr, allResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the parameter file\n",
    "\n",
    "Parameters stored in a CSV file are read and the model generator will be run for each set of parameters.\n",
    "\n",
    "Update the string infile to change the input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300, 2, 2048, 0.001, 200, 10, 'Adam', 0.85]\n"
     ]
    }
   ],
   "source": [
    "allruns = pd.read_csv(indir + infile + \".csv\",sep=\",\")\n",
    "\n",
    "print(allruns.T[0].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is block to execute runs\n",
    "\n",
    "## One run for each line in parameter file:\n",
    "\n",
    "##### read parameters from each line of file, and display parameters being used\n",
    "##### instantiate a model according to parameters (use GPU if available)\n",
    "##### set up optimizer and fold generator according to parameters\n",
    "##### Call RunFolds to execute all the folds\n",
    "##### accumulate all results so they can be saved to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing run number 0\n",
      "Optimiser Adam Using 2 hidden_layers with 300 units in first layer. Batch size of 2048 for 200 epochs. 10 folds. Learning rate 1.0e-03 Alpha 0.85\n",
      "\n",
      "Results Accuracy: mean 0.947 std 0.004 Validation Accuracy: mean 0.907 std 0.009 Runtime 58.32 seconds\n",
      "\n",
      "\n",
      " Al runs run\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "allresults = []\n",
    "\n",
    "for index,row in allruns.iterrows():\n",
    "    num_units = int(row['num_units'])\n",
    "    hidden_layers = int(row['hidden_layers'])\n",
    "    batch_size = int(row['batch_size'])\n",
    "    lr = row['lr']\n",
    "    epochs = int(row['epochs'])\n",
    "    fold_splits = int(row['fold_splits'])\n",
    "    optName = row['optName']\n",
    "    alpha = row['alpha']\n",
    "    \n",
    "    blurb = False\n",
    "    \n",
    "    print(\"Executing run number\", count)\n",
    "    print('Optimiser {} Using {} hidden_layers with {} units in first layer. Batch size of {} for {} epochs. \\\n",
    "{} folds. Learning rate {:.1e} Alpha {:.2f}'.format(\n",
    "            optName, hidden_layers, num_units, batch_size, epochs, fold_splits, lr, alpha))\n",
    "    print()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = torch.nn.DataParallel(TestModel(num_units=num_units, hidden_layers=hidden_layers)).cuda()\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        model = TestModel()\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    if blurb:\n",
    "        print(model)\n",
    "\n",
    "    if optName == \"Wame\":\n",
    "        opt = wame(model.parameters(), lr=lr, alpha=alpha)\n",
    "    if optName == \"SGD\":\n",
    "        opt = SGD(model.parameters(), lr=lr)\n",
    "    if optName == \"Adam\":\n",
    "        opt = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    loss = CrossEntropyLoss().to(device)\n",
    "\n",
    "    nfolds = StratifiedKFold(n_splits=fold_splits, shuffle=True)\n",
    "    \n",
    "    starttime = time.perf_counter()\n",
    "    foldAcc, foldVal, foldErr, tables = RunFolds(model, opt, epochs, nfolds, batch_size, trainX, trainY, blurb)\n",
    "    runtime= time.perf_counter() - starttime\n",
    "    \n",
    "    print('Results Accuracy: mean {:.3f} std {:.3f} Validation Accuracy: mean {:.3f} std {:.3f} \\\n",
    "Runtime {:.2f} seconds'.format(\n",
    "        np.mean(foldAcc), np.std(foldAcc), np.mean(foldVal), np.std(foldVal), runtime) )\n",
    "    print()\n",
    "    print()      \n",
    "    \n",
    "    allresults.append(allruns.T[count].to_list() + foldAcc + foldVal + [runtime] + tables )\n",
    "          \n",
    "    count += 1\n",
    "    \n",
    "print(\" Al runs run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store \n",
    "\n",
    "### all results into a separate CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(indir + \"Z\" + infile + 'Results.csv', 'w+') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows( allresults )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXECUTE TO HERE WHEN RUNNING EXPERIMENTS\n",
    "\n",
    "## Only run final Test when a model has been selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST on true test dataset\n",
    "\n",
    "When a suitable candidate model has been built test it on the test dataset\n",
    "\n",
    "This should only be run when all candidate model parameters have been assessed using the kFold runs specified. Frequent use of the test data will corrupt the parameter selection and leak information from the test data into the final model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class \t Cases \t TP \t FP \t Precision \t Recall \t F1\n",
      "0 \t 1072 \t 1069 \t 4 \t 0.996 \t \t 0.997 \t \t 0.997\n",
      "1 \t 479 \t 478 \t 0 \t 1.0 \t \t 0.998 \t \t 0.999\n",
      "2 \t 961 \t 936 \t 87 \t 0.915 \t \t 0.974 \t \t 0.944\n",
      "3 \t 415 \t 327 \t 53 \t 0.861 \t \t 0.788 \t \t 0.823\n",
      "4 \t 470 \t 455 \t 10 \t 0.978 \t \t 0.968 \t \t 0.973\n",
      "5 \t 1038 \t 981 \t 35 \t 0.966 \t \t 0.945 \t \t 0.955\n",
      "Total \t 4435 \t 4246 \t 189 \t 0.953 \t \t 0.945 \t \t 0.948\n",
      "\n",
      "Class \t Cases \t TP \t FP \t Precision \t Recall \t F1\n",
      "0 \t 461 \t 460 \t 8 \t 0.983 \t \t 0.998 \t \t 0.99\n",
      "1 \t 224 \t 217 \t 7 \t 0.969 \t \t 0.969 \t \t 0.969\n",
      "2 \t 397 \t 372 \t 51 \t 0.879 \t \t 0.937 \t \t 0.907\n",
      "3 \t 211 \t 144 \t 45 \t 0.762 \t \t 0.682 \t \t 0.72\n",
      "4 \t 237 \t 219 \t 23 \t 0.905 \t \t 0.924 \t \t 0.914\n",
      "5 \t 470 \t 406 \t 48 \t 0.894 \t \t 0.864 \t \t 0.879\n",
      "Total \t 2000 \t 1818 \t 182 \t 0.899 \t \t 0.896 \t \t 0.897\n",
      "Test accuracy: 0.957384 validation accuracy: 0.909000\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('initial_weights.state'))\n",
    "\n",
    "normTrainX, normTestX = normalizeData(trainX, testX)\n",
    "\n",
    "trainX_tensor = torch.from_numpy(normTrainX).float()\n",
    "trainY_tensor = torch.from_numpy(trainY).float()\n",
    "\n",
    "valX_tensor = torch.from_numpy(normTestX).float()\n",
    "valY_tensor = torch.from_numpy(testY).float()\n",
    "\n",
    "\n",
    "time00 = time.perf_counter()\n",
    "torch.manual_seed(7)\n",
    "\n",
    "acc, val, err = RunModel(model, opt, epochs, batch_size, trainX_tensor, trainY_tensor, trainX_tensor, trainY_tensor)\n",
    "\n",
    "time01 = time.perf_counter()\n",
    "\n",
    "train_pred, train_score = Check(trainX_tensor, trainY_tensor, model)\n",
    "PrintResults(train_pred, trainY_tensor, True)\n",
    "\n",
    "print()\n",
    "\n",
    "val_pred, val_score = Check(valX_tensor, valY_tensor, model)\n",
    "PrintResults(val_pred, valY_tensor, True)\n",
    "\n",
    "print('Test accuracy: {:.6f} validation accuracy: {:.6f}'.format(\n",
    "        train_score, val_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotrange = epochs\n",
    "\n",
    "plt.plot(range(plotrange), acc[0:plotrange], color='green')\n",
    "plt.plot(range(plotrange), val[0:plotrange], color='blue')\n",
    "plt.plot(range(plotrange), err[0:plotrange], color='r')\n",
    "title = \"Optimizer {} reached accuracy {:.3f} in {:.2f} seconds\".format(optName, train_score, time01 - time00)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy, Validation and Error')\n",
    "plt.title(title)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
