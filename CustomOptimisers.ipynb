{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5, padding=(2,2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5, padding=(2,2))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.fc1 = nn.Linear(3136, 1000)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        y = self.relu1(y)\n",
    "        y = self.pool1(y)\n",
    "        y = self.conv2(y)\n",
    "        y = self.relu2(y)\n",
    "        y = self.pool2(y)\n",
    "        y = self.dropout1(y)\n",
    "        y = y.view(y.shape[0], -1)\n",
    "        y = self.fc1(y)\n",
    "        y = self.relu3(y)\n",
    "        y = self.dropout2(y)\n",
    "        y = self.fc2(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.datasets import mnist\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "torch.cuda.device_count()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "\n",
    "class wame(Optimizer):\n",
    "\n",
    "\n",
    "    def __init__(self, params,lr=1e-3, alpha=0.9, etas=(0.1, 1.2), zetas=(0.01, 100), epsilon=1e-10, debug=False ):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= alpha:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(alpha))\n",
    "        if not 0.0 < etas[0] < 1.0 < etas[1]:\n",
    "            raise ValueError(\"Invalid eta values: {}, {}\".format(etas[0], etas[1]))\n",
    "\n",
    "        defaults = dict(alpha=alpha, etas=etas, zetas=zetas, lr=lr, epsilon=epsilon, debug=debug)\n",
    "        super(wame, self).__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "   \n",
    "        for group in self.param_groups:    \n",
    "            etaminus, etaplus = group['etas']\n",
    "            zeta_min, zeta_max = group['zetas']\n",
    "            alpha = group['alpha']\n",
    "            lr = group['lr']\n",
    "            epsilon = group['epsilon']\n",
    "            debug = group['debug']\n",
    "            _p = 0\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('wame does not support sparse gradients')\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['prev'] = torch.ones_like(p.data, memory_format=torch.preserve_format)\n",
    "                    state['prev'] = grad\n",
    "                    state['Theta'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n",
    "                    state['Z'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n",
    "                    state['zeta'] = torch.ones_like(p.data, memory_format=torch.preserve_format)\n",
    "                    state['gradmult'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n",
    "                    \n",
    "                    state['step_size'] = grad.new().resize_as_(grad).fill_(group['lr']) ### check on initialisation\n",
    "                    state['prev'] = grad\n",
    "                    \n",
    "                if (_p == 0) & debug:\n",
    "                    print(\"Weights at start: \",p.data.sum(), p.data.size())\n",
    "                step_size = state['step_size']\n",
    "                Theta = state['Theta']\n",
    "                Z = state['Z']\n",
    "                zeta = state['zeta']\n",
    "                gradmult = state['gradmult']                    \n",
    "                \n",
    "                state['step'] += 1\n",
    "\n",
    "                gradmult = grad.mul(state['prev'])\n",
    "                zeta[gradmult.gt(0.)] = zeta[gradmult.gt(0.)].mul(etaplus).clamp(zeta_min, zeta_max)\n",
    "                zeta[gradmult.lt(0.)] = zeta[gradmult.lt(0.)].mul(etaminus).clamp(zeta_min, zeta_max)\n",
    "                zeta[gradmult.eq(0.)] = 1\n",
    "                \n",
    "                Z = Z.mul(alpha).add(zeta.mul(1 - alpha))\n",
    "                Theta = Theta.mul(alpha).add(grad.mul(grad).mul(1 - alpha))\n",
    "\n",
    "                step_size = Z.mul(-lr).mul(grad).div(Theta.add(epsilon))\n",
    "\n",
    "                grad = grad.clone(memory_format=torch.preserve_format)\n",
    "                \n",
    "                # update parameters\n",
    "                p.data = p.data.add(step_size)\n",
    "                \n",
    "                if (_p == 0) & debug:\n",
    "                    print(\"Step number \", state['step'])\n",
    "                    print(\"Weights after update: \",p.data.sum(), p.data.size())\n",
    "                    print(\"Updates applied: \",step_size.sum(), step_size.size())\n",
    "                    print(\"Theta: \",Theta.sum())\n",
    "                    print(\"Z: \",Z.sum())\n",
    "                    print(\"zeta: \",zeta.sum())\n",
    "                    print(\"grad: \",grad.sum())\n",
    "                    print(\"grad_sqd\", grad_sqd.sum())\n",
    "                    print(\"gradmult\", gradmult.sum())\n",
    "                state['prev'] = grad\n",
    "                \n",
    "                _p = _p + 1\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 28, 28]             832\n",
      "            Conv2d-2           [-1, 32, 28, 28]             832\n",
      "              ReLU-3           [-1, 32, 28, 28]               0\n",
      "              ReLU-4           [-1, 32, 28, 28]               0\n",
      "         MaxPool2d-5           [-1, 32, 14, 14]               0\n",
      "         MaxPool2d-6           [-1, 32, 14, 14]               0\n",
      "            Conv2d-7           [-1, 64, 14, 14]          51,264\n",
      "            Conv2d-8           [-1, 64, 14, 14]          51,264\n",
      "              ReLU-9           [-1, 64, 14, 14]               0\n",
      "        MaxPool2d-10             [-1, 64, 7, 7]               0\n",
      "             ReLU-11           [-1, 64, 14, 14]               0\n",
      "          Dropout-12             [-1, 64, 7, 7]               0\n",
      "        MaxPool2d-13             [-1, 64, 7, 7]               0\n",
      "          Dropout-14             [-1, 64, 7, 7]               0\n",
      "           Linear-15                 [-1, 1000]       3,137,000\n",
      "           Linear-16                 [-1, 1000]       3,137,000\n",
      "             ReLU-17                 [-1, 1000]               0\n",
      "          Dropout-18                 [-1, 1000]               0\n",
      "             ReLU-19                 [-1, 1000]               0\n",
      "           Linear-20                   [-1, 10]          10,010\n",
      "          Dropout-21                 [-1, 1000]               0\n",
      "            Model-22                   [-1, 10]               0\n",
      "           Linear-23                   [-1, 10]          10,010\n",
      "            Model-24                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 6,398,212\n",
      "Trainable params: 6,398,212\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.39\n",
      "Params size (MB): 24.41\n",
      "Estimated Total Size (MB): 25.80\n",
      "----------------------------------------------------------------\n",
      "DataParallel(\n",
      "  (module): Model(\n",
      "    (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (relu1): ReLU()\n",
      "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (relu2): ReLU()\n",
      "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (dropout1): Dropout(p=0.25, inplace=False)\n",
      "    (fc1): Linear(in_features=3136, out_features=1000, bias=True)\n",
      "    (relu3): ReLU()\n",
      "    (dropout2): Dropout(p=0.5, inplace=False)\n",
      "    (fc2): Linear(in_features=1000, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "batch_size = 1024\n",
    "path = '/home/bernard/Documents/BBK/CourseWork/ML/LeNet5-MNIST-PyTorch/'\n",
    "train_dataset = mnist.MNIST(root=path + 'train', train=True, transform=ToTensor())\n",
    "test_dataset = mnist.MNIST(root=path + 'test', train=False, transform=ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
    "\n",
    "model = Model()\n",
    "model = torch.nn.DataParallel(model).cuda()\n",
    "summary(model, (1, 28, 28))\n",
    "print(model)\n",
    "\n",
    "\n",
    "opt = wame(model.parameters(), lr=1e-7)\n",
    "loss = CrossEntropyLoss()\n",
    "epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  accuracy: 0.912500 time 1.492 seconds\n",
      "epoch: 2  accuracy: 0.935200 time 1.447 seconds\n",
      "epoch: 3  accuracy: 0.937800 time 1.457 seconds\n",
      "epoch: 4  accuracy: 0.942500 time 1.450 seconds\n",
      "epoch: 5  accuracy: 0.945800 time 1.449 seconds\n",
      "epoch: 6  accuracy: 0.944400 time 1.484 seconds\n",
      "epoch: 7  accuracy: 0.947200 time 1.464 seconds\n",
      "epoch: 8  accuracy: 0.949500 time 1.459 seconds\n",
      "epoch: 9  accuracy: 0.952500 time 1.525 seconds\n",
      "epoch: 10  accuracy: 0.954100 time 1.512 seconds\n",
      "epoch: 11  accuracy: 0.955700 time 1.534 seconds\n",
      "epoch: 12  accuracy: 0.954100 time 1.500 seconds\n",
      "epoch: 13  accuracy: 0.956700 time 1.483 seconds\n",
      "epoch: 14  accuracy: 0.958700 time 1.444 seconds\n",
      "epoch: 15  accuracy: 0.959900 time 1.445 seconds\n",
      "epoch: 16  accuracy: 0.958600 time 1.480 seconds\n",
      "epoch: 17  accuracy: 0.960600 time 1.478 seconds\n",
      "epoch: 18  accuracy: 0.959600 time 1.453 seconds\n",
      "epoch: 19  accuracy: 0.961300 time 1.458 seconds\n",
      "epoch: 20  accuracy: 0.962400 time 1.439 seconds\n"
     ]
    }
   ],
   "source": [
    "time0 = time.perf_counter()\n",
    "torch.manual_seed(7)\n",
    "for _epoch in range(epoch):\n",
    "    batch = 0\n",
    "    for idx, (train_x, train_label) in enumerate(train_loader):\n",
    "        train_x = train_x.cuda()\n",
    "        train_label = train_label.cuda()\n",
    "        label_np = np.zeros((train_label.shape[0], 10))\n",
    "        opt.zero_grad()\n",
    "        predict_y = model(train_x.float())\n",
    "        _error = loss(predict_y, train_label.long())\n",
    "        _error.backward()\n",
    "        opt.step()\n",
    "\n",
    "        batch = batch + 1\n",
    "    \n",
    "    correct = 0\n",
    "    _sum = 0\n",
    "\n",
    "    for idx, (test_x, test_label) in enumerate(test_loader):\n",
    "        test_x = test_x.cuda()\n",
    "        test_label = test_label\n",
    "        predict_y = model(test_x.float()).detach().cpu()\n",
    "        predict_ys = np.argmax(predict_y, axis=-1)\n",
    "        label_np = test_label.numpy()\n",
    "        _ = predict_ys == test_label\n",
    "        correct += np.sum(_.numpy(), axis=-1)\n",
    "        _sum += _.shape[0]\n",
    "\n",
    "    time1 = time.perf_counter()\n",
    "    print('epoch: {}  accuracy: {:.6f} time {:.3f} seconds'.format(_epoch + 1, correct / _sum, time1 - time0))\n",
    "    time0 = time1\n",
    "    #torch.save(model, path + 'models/mnist_{:.2f}.pkl'.format(correct / _sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0\n",
      "10.2\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
